{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Citation\n",
    "# F. Maxwell Harper and Joseph A. Konstn. 2015. The MovieLens Datasets: History and Context. \n",
    "! curl https://grouplens.org/datasets/movielens/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abosede/PROJECTS/DATA SCIENCE/RECOMMENDER SYSTEMS/Customer-focused-movie-recommender-systems\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to unzip our file\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/Users/abosede/PROJECTS/DATA SCIENCE/RECOMMENDER SYSTEMS/Customer-focused-movie-recommender-systems/ml-latest-small.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "movies_df = pd.read_csv('/Users/abosede/PROJECTS/DATA SCIENCE/RECOMMENDER SYSTEMS/Customer-focused-movie-recommender-systems/data/ml-latest-small/movies.csv')\n",
    "ratings_df = pd.read_csv('/Users/abosede/PROJECTS/DATA SCIENCE/RECOMMENDER SYSTEMS/Customer-focused-movie-recommender-systems/data/ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the movies dataframe are: (9742, 3) \n",
      "The dimensions of ratings dataframe are: (100836, 4)\n"
     ]
    }
   ],
   "source": [
    "print('The dimensions of the movies dataframe are:', movies_df.shape, '\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick look at the movies data frame (movies_df)\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick look at the ratings data frame\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "Number of unique movies 9724\n",
      "Number of unique users (2): 610\n",
      "The full rating matrix will have: 5931640 elements.\n",
      "----------\n",
      "Number of ratings: 100836\n",
      "Therefore:  1.6999683055613624 % of the matrix is filled.\n",
      "We have an incredibly sparse matrix to work with here\n",
      "We can imagine as the number of users and products grow, the number of elements will increase by n*2\n",
      "We are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\n",
      "One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\n"
     ]
    }
   ],
   "source": [
    "# This is to create a Movie ID to movie name mapping\n",
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "# From the data frame this brings out \"movieId\" as the index column. Then the title is what to be returned\n",
    "# The generated series is then converted to a dictionary instead, that is the movie id and title become key - value mappings\n",
    "\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "\n",
    "n_users_1 = len(ratings_df[\"userId\"].unique())\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique movies\", n_items)\n",
    "print(\"Number of unique users (2):\", n_users_1)\n",
    "\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "\n",
    "print('----------')\n",
    "print('Number of ratings:', len(ratings_df))\n",
    "print(\"Therefore: \", len(ratings_df)/ (n_users * n_items) * 100, '% of the matrix is filled.')\n",
    "\n",
    "print('We have an incredibly sparse matrix to work with here')\n",
    "print('We can imagine as the number of users and products grow, the number of elements will increase by n*2')\n",
    "print('We are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.')\n",
    "print('One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don\\'t need all the data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to install the needed libraries -> We would only be using pytorch here\n",
    "%pip install torch torchvision torchaudio\n",
    "# torch -> the core Pytorch library\n",
    "# torchvision -> for image-related tasks (like datasets and transforms)\n",
    "# torchaudio -> for audio processing (if needed!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /Users/abosede/Library/Python/3.13/lib/python/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm # This is for progress bars in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm  #as tqdm\n",
    "# tqdm is a Python library that adds progress bars to loops - super userful for tracking long-running tasks\n",
    "# tqdm_notebook : This is a variant that displays the progess bar nicely inside a Jupyter Notebook interface\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors = 20):\n",
    "        super().__init__() # This is to set up our parent class -> \"torch.nn.Module\"\n",
    "        \n",
    "        # For the embedding layer\n",
    "        '''\n",
    "            This creates a lookup table that maps each user ID to a dense vector of size \"n_factors\"ArithmeticError\n",
    "            n_users: This is is the number of unique users - each one gets its own vector\n",
    "            n_factors: The size of each embedding vector - this defines the dimensionality of the latent space\n",
    "\n",
    "            So, if we have 1000 users and \"n_factors = 20\", this layer will learn a \"1000 x 20\" matrix during training\n",
    "\n",
    "            # Similar users will have similar embeddings and then similar items will have similar embeddings\n",
    "        '''\n",
    "        # Create the user embedding layer -> This would later be called on the actual data\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # Think of this as a look up table for the input\n",
    "\n",
    "        # Create the item embedding layer\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # Think of this as a lookup table for the input\n",
    "\n",
    "        # Further explanation\n",
    "        '''\n",
    "        .weight -> This accesses the actual weight matrix - the lookup table of user embeddings\n",
    "        .data -> Refers to the raw tensor data (bypassing autograd tracking)\n",
    "        .uniform_(0,0.05) -> Fills the tensor with random values uniformly distributed between 0 and 0.05\n",
    "        '''\n",
    "        # Why???\n",
    "        '''\n",
    "         Initially, PyTorch initializes weights randomly, but you can override that to control the range\n",
    "         Initializing with small values like this can help:\n",
    "            Prevent exploding gradients early in training\n",
    "            Ensure embeddings start close to zero, which can stabilize learning\n",
    "            Avoid biasing the model toward any particular direction before training begins\n",
    "        '''\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0,0.05)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "\n",
    "        # Note\n",
    "        '''\n",
    "            data[:,0] -> This grabs all rows in the first column\n",
    "        '''\n",
    "        # Multiply the matrices\n",
    "        return (self.user_factors(users) * self.item_factors(items)).sum(1)\n",
    "\n",
    "        '''\n",
    "            self.user_factors(users) -> This retrieves the embedding vector for each userID in the users tensor\n",
    "            * -> This multiplies each user vector with its corresponding item vector — element by element. Still shape: [batch_size, n_factors]\n",
    "            .sum(1) -> This sums across the embedding dimensions (n_factors) for each user-item pair, producing a single scalar score per pair. Final shape: [batch_size]\n",
    "        '''\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearing the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # This is the package that helps transform the data to machine learning readiness\n",
    "\n",
    "# Note: This is not good practice, in a MLOps sense but we'll roll with this since the data is already loaded in memory\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "\n",
    "        # Extract all user IDs and movie IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "\n",
    "        # Producing new continuous IDs for users and movies --\n",
    "\n",
    "        # This is like forward mapping (That is, creating a mapping for the movieID and the actual ID)\n",
    "        # Unique values : index\n",
    "        self.userid2idx  = {o:i for i, o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "\n",
    "        # Obtained continuous ID for users and movies\n",
    "        # Then to map backwards -> That is, \n",
    "        self.idx2userid = {i:o for o, i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "\n",
    "        # return the id from the indexed values as nootes in the lambda function down below\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x : self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "\n",
    "        # Then to classify the predictor and target!\n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis =1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "\n",
    "        # Then to convert the columns into model ready columns\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "\n",
    "    '''\n",
    "        These are helper functions\n",
    "    '''\n",
    "\n",
    "    # This is to get the item (From the tensor) -> Given an index\n",
    "    # How this function can be used\n",
    "    # Say we instantiate like : loader = Loader()\n",
    "    # Then we can have: item = loader[index]\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index]) \n",
    "    \n",
    "    # How can this be called?? \n",
    "    # Say we have the same above, then we can call it like\n",
    "    # len(loader) -> This will directly trigger this function\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(610, 8)\n",
      "  (item_factors): Embedding(9724, 8)\n",
      ")\n",
      "user_factors.weight tensor([[2.0839e-02, 4.1836e-02, 3.3281e-02,  ..., 4.2128e-04, 1.2003e-02,\n",
      "         2.9348e-02],\n",
      "        [1.8641e-02, 1.3165e-02, 1.2323e-02,  ..., 4.7656e-02, 4.1534e-03,\n",
      "         4.6812e-03],\n",
      "        [1.8489e-02, 8.4038e-03, 3.6098e-02,  ..., 3.5817e-02, 4.5381e-02,\n",
      "         1.9939e-02],\n",
      "        ...,\n",
      "        [4.9437e-02, 3.6056e-02, 2.1113e-02,  ..., 9.9880e-05, 2.5980e-04,\n",
      "         4.7079e-02],\n",
      "        [3.8748e-02, 4.1664e-02, 3.4890e-03,  ..., 2.0833e-02, 2.7234e-02,\n",
      "         4.6142e-02],\n",
      "        [4.8907e-02, 1.9885e-02, 5.5604e-03,  ..., 2.9974e-02, 2.8602e-03,\n",
      "         3.9468e-02]])\n",
      "item_factors.weight tensor([[0.0103, 0.0019, 0.0363,  ..., 0.0275, 0.0189, 0.0368],\n",
      "        [0.0316, 0.0391, 0.0291,  ..., 0.0018, 0.0110, 0.0106],\n",
      "        [0.0116, 0.0109, 0.0328,  ..., 0.0350, 0.0366, 0.0275],\n",
      "        ...,\n",
      "        [0.0378, 0.0313, 0.0192,  ..., 0.0245, 0.0191, 0.0378],\n",
      "        [0.0034, 0.0460, 0.0096,  ..., 0.0479, 0.0448, 0.0004],\n",
      "        [0.0037, 0.0475, 0.0404,  ..., 0.0463, 0.0076, 0.0317]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Calls __len__() to know how many samples are in train_set.\\n    Randomly shuffles the indices (if shuffle=True).\\n    Uses __getitem__() to fetch 128 samples at a time (your batch size).\\n\\n    1. __len__(self)\\n        Returns the total number of samples in your dataset.\\n\\n        Used to determine how many batches to create.\\n\\n    2. __getitem__(self, index)\\n        Returns a single data sample (and optionally its label) at the given index.\\n\\n        This is what gets batched together during training.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 128 # This sets the number of training epochs — how many times your model will see the entire dataset during training.\n",
    "cuda = torch.cuda.is_available() # This checks if a CUDA-compatible GPU is available on your system.\n",
    "# CUDA is NVIDIA’s parallel computing platform that allows PyTorch to run operations on the GPU, which is much faster than CPU for deep learning tasks.\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "\n",
    "# Then to instantiate our model\n",
    "model = MatrixFactorization(n_users, n_items, n_factors = 8)\n",
    "\n",
    "print(model) # this prints out the super class's __str__ function\n",
    "\n",
    "'''\n",
    "\".named_parameters()\" -> comes from the \"nn.Module\" class - from which \"MatrixFactorization\" is inheriting from\n",
    ".named_parameters() -> This returns an iterator over (name, parameter) pairs for all parameters in the model that have \"requires_grad=True\" \n",
    "These parameters are typically the weights and biases of the layers we have defined\n",
    "In this case\n",
    "The model has:\n",
    "user_factors: nn.Embedding(610,8) layer\n",
    "item_factors: nn.Embedding(9724,8) layer\n",
    "\n",
    "Each of these layers has a \".weight\" tensor - that's what we are accessing\n",
    "\n",
    "## param.requires_grad\n",
    "-- This checks whether the parameter should be updated during backpropagation. If \"True\", it means the parameter is learnable - gradients will be computed for it and it will be updated during training\n",
    "'''\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "# GPU enable if we have one! -> In this example, I do not have so I am not going to be doing that\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "# MSE loss\n",
    "# This is the mean square error loss\n",
    "# This calculates how far off your model's predictions are from the actual values\n",
    "# The goal is to minimize this loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "'''\n",
    "Adam stands for Adaptive Moment Estimation — it’s a popular optimizer that combines the benefits of SGD and RMSProp.\n",
    "It adjusts learning rates for each parameter individually based on estimates of first and second moments of the gradients.\n",
    "model.parameters() tells it which weights to update — in your case, the embeddings for users and items.\n",
    "lr=1e-3 sets the learning rate, which controls how big each update step is. A smaller value means slower but more stable learning.\n",
    "'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "\n",
    "# Train the data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle = True)\n",
    "\n",
    "# What happens under the hood\n",
    "'''\n",
    "    Calls __len__() to know how many samples are in train_set.\n",
    "    Randomly shuffles the indices (if shuffle=True).\n",
    "    Uses __getitem__() to fetch 128 samples at a time (your batch size).\n",
    "\n",
    "    1. __len__(self)\n",
    "        Returns the total number of samples in your dataset.\n",
    "\n",
    "        Used to determine how many batches to create.\n",
    "\n",
    "    2. __getitem__(self, index)\n",
    "        Returns a single data sample (and optionally its label) at the given index.\n",
    "\n",
    "        This is what gets batched together during training.\n",
    "'''\n",
    "\n",
    "# Note :\n",
    "# This DataLoader(., ., .) returns the batches of data that you can loop through in training\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Since I don't have a GPU now, I will be using my device instead!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Then to actually train the model\n",
    "for it in tqdm(range(num_epochs)): # This tqdm here is to show a form of percentage of completion\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "        if not cuda: # Since we do not have the GPU available\n",
    "\n",
    "            #x, y = x.cuda(), y.cuda()\n",
    "            # Since we do not have a GPU\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # This clears the old gradients from the previous step!\n",
    "            # Without this, gradients would accumulate across batches, which would mess up training\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "\n",
    "            ''' \n",
    "                We're implicitly calling the forward() method of your MatrixFactorization class.\n",
    "                This is a PyTorch convention — when we call a model like a function, PyTorch automatically routes that call to the model’s forward() method.\n",
    "            '''\n",
    "            ''' \n",
    "                The __call__() method of nn.Module is overridden to internally call forward() — so when you do model(x), it’s really doing:\n",
    "                def __call__(self, *input, **kwargs):\n",
    "                    return self.forward(*input, **kwargs)\n",
    "            '''\n",
    "            outputs = model(x)\n",
    "           \n",
    "            ''' \n",
    "            Compares the model’s predictions (outputs) to the true labels (y) using Mean Squared Error.\n",
    "            .squeeze() -> removes extra dimensions from outputs if needed.\n",
    "            y.type(torch.float32) ->  ensures the target is in the correct format for MSELoss.\n",
    "            '''\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "\n",
    "            # Adds the scalar loss value to a list so you can track average loss for the epoch.\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            # Computes gradients of the loss with respect to model parameters.\n",
    "            # This is the core of backpropagation.\n",
    "            '''  \n",
    "                PyTorch traces the computation graph from the loss all the way back to the model’s parameters.\n",
    "                It calculates the gradients — how much each parameter contributed to the error.\n",
    "                These gradients are stored in each parameter’s .grad attribute.\n",
    "                Without loss.backward(), your model has no idea how to improve\n",
    "            '''\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            ''' \n",
    "                Uses the gradients to update the model’s parameters.\n",
    "                In your case, it adjusts the user and item embeddings to better predict ratings.\n",
    "                This uses the gradients to update the weights.\n",
    "                So the next time you run a forward pass, the model should (hopefully) make better predictions.\n",
    "            '''\n",
    "            optimizer.step()\n",
    "    ''' \n",
    "    After all batches in the epoch are processed, it prints the average loss.\n",
    "    This gives you a sense of how well the model is learning over time.\n",
    "    '''      \n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses)) # \"it\" here is the current epoch number\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[ 1.4220,  1.2288,  0.5815,  ...,  1.4883,  1.5858,  1.0978],\n",
      "        [ 1.9661,  1.4178,  0.4871,  ...,  0.8738,  0.7888,  1.1253],\n",
      "        [-0.5772,  0.2667,  3.0283,  ..., -0.9325,  0.6023,  1.7131],\n",
      "        ...,\n",
      "        [ 1.0387,  1.0115,  1.3287,  ..., -0.0240, -0.4031,  2.5860],\n",
      "        [ 1.2045,  0.8297,  0.9233,  ...,  1.2070,  1.7543,  0.7149],\n",
      "        [ 0.4663,  0.3207,  1.4207,  ...,  1.3703,  1.4531,  1.7608]])\n",
      "item_factors.weight tensor([[ 0.4195,  0.1586,  0.7142,  ...,  0.6684,  0.4013,  0.1998],\n",
      "        [ 0.6031,  0.4752,  0.5066,  ...,  0.4202,  0.7134, -0.0458],\n",
      "        [ 0.2002,  0.5248,  0.5411,  ...,  0.4468,  0.5227,  0.8026],\n",
      "        ...,\n",
      "        [ 0.3484,  0.2932,  0.3300,  ...,  0.3342,  0.3295,  0.3483],\n",
      "        [ 0.3907,  0.4831,  0.4033,  ...,  0.4410,  0.4382,  0.3949],\n",
      "        [ 0.3654,  0.3850,  0.4069,  ...,  0.4131,  0.3748,  0.3985]])\n"
     ]
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for movies and users\n",
    "c = 0\n",
    "uw = 0 # This is the user's weight \n",
    "iw = 0 # This is the item's weight\n",
    "\n",
    "''' \n",
    "On the first iteration (c == 0), it stores the first parameter’s data in uw (likely the user embeddings).\n",
    "\n",
    "On the second iteration, it stores the next parameter’s data in iw (likely the item embeddings).\n",
    "\n",
    "It prints the name and raw tensor values of each parameter.\n",
    "'''\n",
    "for name, param in model.named_parameters(): # Same thing we did before\n",
    "    if param.requires_grad: # This is to check if it requires a gradient\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "            uw = param.data\n",
    "            c += 1\n",
    "        else:\n",
    "            iw = param.data\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then to check the trained movie embeddings\n",
    "''' \n",
    "    model.item_factors.weight : Accesses the raw weight matrix of the item embedding layer — shape: [n_items, n_factors]\n",
    "    .data : Gets the actual tensor data (bypassing autograd tracking)\n",
    "    .cpu() : Moves the tensor from GPU to CPU (important if you're using CUDA)\n",
    "    .numpy() : Converts the PyTorch tensor into a NumPy array\n",
    "'''\n",
    "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4195042 ,  0.15856628,  0.714191  , ...,  0.66836995,\n",
       "         0.4013267 ,  0.19979613],\n",
       "       [ 0.60311675,  0.47524107,  0.5065975 , ...,  0.4201884 ,\n",
       "         0.713357  , -0.04575742],\n",
       "       [ 0.20023791,  0.5248257 ,  0.54112893, ...,  0.44683835,\n",
       "         0.5226808 ,  0.8025828 ],\n",
       "       ...,\n",
       "       [ 0.3483773 ,  0.29323405,  0.329968  , ...,  0.33417743,\n",
       "         0.32953516,  0.34830368],\n",
       "       [ 0.39071354,  0.4831054 ,  0.40333667, ...,  0.44097462,\n",
       "         0.4382096 ,  0.39491692],\n",
       "       [ 0.36535263,  0.38500917,  0.40688872, ...,  0.4131225 ,\n",
       "         0.37478933,  0.39847574]], shape=(9724, 8), dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_movie_embeddings # Display the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state = 0).fit(trained_movie_embeddings)\n",
    "# Here, we are clustering the learned movie embeddings usu=ing the K-Means algorithm - This is a classic unsupervised learning technique\n",
    "'''\n",
    "KMeans(...): Initializes the K-Means clustering model.\n",
    "\n",
    "n_clusters=10: We’re asking it to group the movies into 10 distinct clusters based on their learned embeddings.\n",
    "\n",
    "random_state=0: Sets a seed for reproducibility — ensures you get the same clusters every time you run it.\n",
    "\n",
    ".fit(trained_movie_embeddings): Applies K-Means to your movie vectors (from the embedding layer).\n",
    "\n",
    "Each movie is represented as a vector in a latent space (e.g., 20-dimensional if n_factors=20).\n",
    "\n",
    "K-Means finds 10 centroids and assigns each movie to the nearest one.\n",
    "'''\n",
    "\n",
    "print(type(movie_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    It can be seen here that the movies that are in the same cluster tend to have similar genres.\n",
    "    Also note that the algorithm is unfamiliar with the movie name\n",
    "    and only obtained the relationships by looking at the numbers representing how users have responded to the movies selection\n",
    "\n",
    "'''\n",
    "for cluster in range(10):\n",
    "    print(\"Cluster #{}\".format(cluster))\n",
    "    movs = []\n",
    "\n",
    "    '''\n",
    "    kmeans.labels contains the cluster assignment for each movie. -> it is a numpy array itself\n",
    "    np.where(...)[0] finds the indices of movies that belong to the current cluster.\n",
    "    movidx is the index of a movie in your embedding array.   \n",
    "    '''\n",
    "    for movidx in np.where(kmeans.labels_ == cluster)[0]: # Check for the movies in the same cluster!\n",
    "        ''' \n",
    "            Here,\n",
    "            We are converting the internal index (movidx) back to the actual movieId used in your dataset.\n",
    "            idx2movieid is a mapping you created earlier to reverse the embedding index. \n",
    "        '''\n",
    "        movid = train_set.idx2movieid[movidx]\n",
    "\n",
    "        ''' \n",
    "          We’re checking how many ratings this movie received in your dataset.\n",
    "          This gives you a measure of popularity or engagement.  \n",
    "        '''\n",
    "        rat_count = ratings_df.loc[ratings_df['movieId'] == movid].count()[0]\n",
    "\n",
    "        ''' \n",
    "            We’re building a list of tuples: (movie title, rating count) for all movies in the cluster\n",
    "        '''\n",
    "        movs.append((movie_names[movid],rat_count))\n",
    "\n",
    "    ''' \n",
    "    tup is just a placeholder name for each element in the list movs.\n",
    "    Since each element is a tuple, tup[1] accesses the second item — the rating count.\n",
    "    This tells sorted() to sort the list based on rating count.\n",
    "    Note that \"tup\" here represents each individual tuple that we have\n",
    "    when you use a lambda function inside something like sorted(), map(), or filter(), you're applying that function to each individual element of the iterable you're working with.\n",
    "    '''\n",
    "    for mov in sorted(movs, key = lambda tup : tup[1], reverse = True)[:10]:\n",
    "        print(\"\\t\", mov[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
